{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WE RATE DOGS PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13760/1898008863.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOAuthHandler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Enhanced Twitter Archive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "print('Archive data has: ' + str(len(archive_df)) + ' observations')\n",
    "archive_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Image Predictions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred_df = pd.read_csv('image-predictions.tsv', sep='\\t')\n",
    "print('Image Predictions data has: ' + str(len(img_pred_df)) + ' observations')\n",
    "img_pred_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Twitter Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file\n",
    "# These are hidden to comply with Twitter's API terms and conditions\n",
    "consumer_key = 'hRwnPAXiqvoWWvBdndaQfVgVC'\n",
    "consumer_secret = '3KyvKdER17GOvY7coL5pZGG0n4iZqXyLAJeS240K8QXWQo1OHd'\n",
    "access_token = '1204094449363673094-Bw8332TZG11wCRfB8L7XzkUUUASBKL'\n",
    "access_secret = 'bHL5BEgFeBXuMlOp4GL0J9bs3GdHmPHyGAIhgqku8ZJaI'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# NOTE TO STUDENT WITH MOBILE VERIFICATION ISSUES:\n",
    "# df_1 is a DataFrame with the twitter_archive_enhanced.csv file. You may have to\n",
    "# change line 17 to match the name of your DataFrame with twitter_archive_enhanced.csv\n",
    "# NOTE TO REVIEWER: this student had mobile verification issues so the following\n",
    "# Twitter API code was sent to this student from a Udacity instructor\n",
    "# Tweet IDs for which to gather additional data via Twitter's API\n",
    "tweet_ids = archive_df.tweet_id.values\n",
    "#len(tweet_ids)\n",
    "\n",
    "# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "start = timer()\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open('tweet_json.txt', 'w') as outfile:\n",
    "    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "    for tweet_id in tweet_ids:\n",
    "        #count += 1\n",
    "        #print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            json.dump(tweet._json, outfile)\n",
    "            outfile.write('\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "print(\"total time: \" + str(end - start))\n",
    "print(\"The following tweet ids have failed:\")\n",
    "print(fails_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for line in open('tweet_json.txt', 'r'):\n",
    "    tweets.append(json.loads(line))\n",
    "scraped_df = pd.DataFrame(tweets)\n",
    "print('Scraped data has: ' + str(len(scraped_df)) + ' observations')\n",
    "scraped_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "We can clearly see immediately that we were not able to retrieve the data for 30 tweet_ids from twitter scraping, hence only 2326 tweets. Also image predictions also has only 2074 tweets. This causes completeness issue in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that in_reply_status_id and retweeted_status_id are non-null for certain rows. Implyig those rows are either replies/retweets. Those are relevant data to our project.\n",
    "\n",
    "There is also 4 different columns for doggo/floofer/pupper/puppo which could one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we need to remove the from replies/retweets from the scraped data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df[archive_df.tweet_id.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred_df[img_pred_df.tweet_id.duplicated()]\n",
    "img_pred_df[img_pred_df.jpg_url.duplicated()]['jpg_url'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that 66 tweet_ids in image predictions data have duplicated jpg_url indicating that these tweet_ids don't give any extra information hence irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df[scraped_df.id.duplicated()]\n",
    "scraped_df[scraped_df.full_text.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retweet and favourites count being 0 seem like outliers or invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rating_numerator being 0 seems suspicious. And the rating_denominator is supposed to be always 10. These need further inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image prediction numerical values look okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Futher Inspection of columns\n",
    "#### Inspecting duplicates in jpg_url\n",
    "Taking sample of three urls from the list of duplicates and visually inspecting those rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_urls_sample = img_pred_df[img_pred_df.jpg_url.duplicated()]['jpg_url'].sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive[archive['rating_denominator'] != 10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
